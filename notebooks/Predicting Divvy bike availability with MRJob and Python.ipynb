{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm going to show you how I've been collecting Divvy bike data with the ultimate goal of getting probability distributions for every bike dock in Chicago for every minute of both weekdays and weekends. My ultimate goal is to create a visualization of some sort to see how availability of both docks and bikes changes over time per station. In this article I'll get to the point where I'll be able to make some predictions about how many bikes are available at this exact minute.\n",
      "\n",
      "What are Divvy bikes? Divvy is a bike sharing system in use throughout Chicago. Every minute they post data regarding the status of every bike dock in the city to an open API. You can learn more about Divvy bikes here: [http://divvybikes.com](http://divvybikes.com)\n",
      "\n",
      "I've been collecting data for the Divvy bikes throughout Chicago since September with my friend Andrew Slotnick. He showed me that it's a pretty simple API to scrape. Just hit this URL and record the results every minute:\n",
      "\n",
      "[http://divvybikes.com/stations/json](http://divvybikes.com/stations/json)\n",
      "\n",
      "I have a Mac Mini in my living that my Fianc\u00e9e watches TV shows on while I'm running processes to scrape data sources. It's all about synergy!\n",
      "\n",
      "So on this multi-tasking Mac Mini I have the following Python script running:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import datetime\n",
      "import os, time, socket, datetime, glob\n",
      "import subprocess\n",
      "\n",
      "socket.setdefaulttimeout(20)\n",
      "\n",
      "def group_previous_days_files():\n",
      "    print \"Attempting grouping...\"\n",
      "    print \"bozo...\"\n",
      "    todays_group = datetime.datetime.now().strftime('%Y%m%d')\n",
      "    print \"Today's group is \" + todays_group\n",
      "    log_filenames = [file_path for file_path in glob.glob(\"./DownloadedData/*.json\") if not todays_group in file_path and not 'day_' in file_path]\n",
      "    print \"Logs found: \" + str(log_filenames)\n",
      "    for log_filename in log_filenames:\n",
      "        print \"Grouping \" + log_filename\n",
      "        base_log_filename = os.path.basename(log_filename)\n",
      "        the_date = base_log_filename.split('_')[0]\n",
      "        grouped_file_path = data_folder + 'day_' + the_date + '.json'\n",
      "        with open(grouped_file_path, 'a') as day_file:\n",
      "            with open(log_filename, 'r') as minute_file:\n",
      "                minute_text = minute_file.read()\n",
      "                day_file.write(minute_text)\n",
      "        print \"Removing \" + log_filename\n",
      "        os.remove(log_filename)\n",
      "    os.system('s3cmd sync ./DownloadedData/day_*.json s3://bozo-divvy/day-log/')\n",
      "    print \"Done uploading to S3\"\n",
      "    for day_file in glob.glob('./DownloadedData/day_*.json'):\n",
      "        os.remove(day_file)\n",
      "\n",
      "data_folder = './DownloadedData/'\n",
      "while True:\n",
      "    current_file_name = datetime.datetime.now().strftime('%Y%m%d_%H%M') + '.json'\n",
      "    current_file_path = data_folder + current_file_name\n",
      "    if not os.path.exists(current_file_path):\n",
      "        print \"Downloading file... then sleeping.\"\n",
      "        try:\n",
      "            urllib.urlretrieve (\"http://divvybikes.com/stations/json\", current_file_path)\n",
      "        except:\n",
      "             time.sleep(3)\n",
      "             continue\n",
      "        group_previous_days_files()\n",
      "    else:\n",
      "        print \"File already exists. Sleeping\"\n",
      "    time.sleep(30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This script does the following:\n",
      "\n",
      "* Hits the Divvy bike service every 30 seconds and downloads the json if it hasn't already been downloaded this minute.\n",
      "* If an error happens during the download the service just swallows it.\n",
      "* After attempting the download I group up the previous days' logs into full day files then upload them to S3.\n",
      "* I delete the no longer necessary minute by minute logs (NOTE: any logs from the current day reamain in minute by minute form).\n",
      "\n",
      "If you watched Hilary Mason in this video you might recognize this as the \"Obtain\" and \"Scrub\" steps of data science:\n",
      "\n",
      "* Dirty secrets of data science: [http://blog.mortardata.com/post/45192413994/dirty-secrets-of-data-science](http://blog.mortardata.com/post/45192413994/dirty-secrets-of-data-science)\n",
      "* Also see dataist here: [http://www.dataists.com/2010/09/a-taxonomy-of-data-science/](http://www.dataists.com/2010/09/a-taxonomy-of-data-science/)\n",
      "\n",
      "This gives data in this format:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "{\"executionTime\":\"2014-01-20 12:07:02 PM\",\"stationBeanList\":[{\"id\":5,\"stationName\":\"State St & Harrison St\",\"availableDocks\":15,\"totalDocks\":19,\"latitude\":41.8739580629,\"longitude\":-87.6277394859,\"statusValue\":\"In Service\",\"statusKey\":1,\"availableBikes\":4,\"stAddress1\":\"State St & Harrison St\",\"stAddress2\":\"\",\"city\":\"\",\"postalCode\":\"\",\"location\":\"620 S. State St.\",\"altitude\":\"\",\"testStation\":false,\"lastCommunicationTime\":null,\"landMark\":\"030\"},{\"id\":13,..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next step is..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exploring the data\n",
      "\n",
      "My goal from the get go with this data has been to answer the question: \"How likely am I to find an available bike or dock?\"\n",
      "\n",
      "I'd also love to be able to see this visualized and animated on a map. We'll see about that. First let's answer my burning question. To do that I'll need to design roughly how I want my data to be organized when I get done with it.\n",
      "\n",
      "What I want is a row of data that lists the bike station, whether or not it's a weekend, the hour, and the minute of the day, as well as a frequency distribution that maps how often X number of bikes and docks were available. Something like this:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "{\n",
      "    \"latitude\": 41.795211999999999, \n",
      "    \"longitude\": -87.580714999999998, \n",
      "    \"station_name\": \"Shore Drive & 55th St\", \n",
      "    \"minute\": 3, \n",
      "    \"weekday\": true\n",
      "    \"hour\": 0, \n",
      "    \"station_id\": 247, \n",
      "    \"available_bikes\": [0, 1, 7, 10, 9, 10, 8, 16, 2, 3, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
      "    \"total_docks\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    \"available_docks\": [0, 0, 1, 0, 0, 6, 3, 2, 16, 8, 10, 9, 10, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's what I want my final result to look like. Each index in the arrays maps to a hypothesis. So for available bikes, zero times there were zero available bikes, one time there was one available bike, seven times there were two available bikes, etc. To come up with a probability we just add up all of the elements of the array and divide each element by that sum. The same applies for available docks and total docks.\n",
      "\n",
      "Two things to keep in mind:\n",
      "\n",
      "1. Just because we've never seen a given number of occurences doesn't mean it's impossible. In other words all the zeroes are a bit extreme. We should add some small amount to each element just to be sure when we do math that zero doesn't count as certainty in impossibility.\n",
      "2. You may have noticed \"total_docks\" above. Technically the number of docks available could change. This allows me to account for that. In the case above, this station always had 15 total docks.\n",
      "\n",
      "So how do we get from the first data format to this ideal one when we've got gigabytes of data being stored on S3? MRJob.\n",
      "\n",
      "I could download all of the files to my local computer and run a script locally to do my aggregation. What's great about MRJob is I can use the same script I write locally to run on my data on S3 using Amazon's Elastic Map Reduce (EMR). So I'll start by downloading some of the data locally and running my script locally and once it seems to be working I'll decide if it's easier to run it locally or in the cloud.\n",
      "\n",
      "Let's get started!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MRJob step by step\n",
      "\n",
      "If you need a MapReduce intro or refresher my good friend Tom Hayden has a great introduction to MRJob where he analyzes chess data. Check it out here: [http://tomhayden3.com/2013/12/27/chess-mr-job/](http://tomhayden3.com/2013/12/27/chess-mr-job/)\n",
      "\n",
      "This example will require us to use some of MRJob's advanced features like multi-step support and protocols.\n",
      "\n",
      "### Step 0: Decide on input and output formats\n",
      "Above I showed that the data I collect from divvy is in the form of JSON. So rather than import lines of text and parse them as JSON into Python dictionaries, I can just have MRJob do that for me with \"protocols\". Below I show what's necessary to define the input and output protocols for this job.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mrjob.job import MRJob\n",
      "from mrjob import protocol\n",
      "\n",
      "class AggregateDivvyFrequencies(MRJob):\n",
      "    INPUT_PROTOCOL = protocol.JSONValueProtocol\n",
      "    OUTPUT_PROTOCOL = protocol.JSONValueProtocol\n",
      "..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main ones I use for input and output are \"protocol.RawValueProtocol\" and \"protocol.JSONValueProtocol\". The RawValueProtocol will pass data through just on the value parameter for input and output and won't add any additional formatting to what you explicitly do. This is the default input protocol for MRJob. To read more about protocols check this link out: [http://pythonhosted.org/mrjob/guides/writing-mrjobs.html#job-protocols](http://pythonhosted.org/mrjob/guides/writing-mrjobs.html#job-protocols)\n",
      "\n",
      "In this job I've decided to make my life easier by reading in the lines of text as JSON objects and I personally prefer saving the data as JSON as well."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 1: organize the raw data\n",
      "\n",
      "First I need to tally for each of my stations how many times they had a given number of bikes, docks, total docks for weekdays and weekends. This essentially means grouping the stations by their pertinent information (lat/long, station id, is_weekday, etc) by setting that info as the key and then setting the value to 1 to act as a tally of how many times I've seen this. Each line of Divvy data includes metadata around the logging (such as the time the data was logged) and ALL the data for each and every station. This means that my mapping step will create many results from the mapper for each line of input.\n",
      "\n",
      "Here's what this step looks like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def organize_station_data_and_tally(self, _, station_update):\n",
      "    execution_time_text = station_update['executionTime']\n",
      "    the_datetime = datetime.strptime(execution_time_text, '%Y-%m-%d %I:%M:%S %p')\n",
      "    the_hour = the_datetime.hour\n",
      "    the_minute = the_datetime.minute\n",
      "    weekday_index = the_datetime.weekday()\n",
      "    weekday_map = [\n",
      "        'Monday',\n",
      "        'Tuesday',\n",
      "        'Wednesday',\n",
      "        'Thursday',\n",
      "        'Friday',\n",
      "        'Saturday',\n",
      "        'Sunday'\n",
      "    ]\n",
      "    station_list = station_update['stationBeanList']\n",
      "    for station_info in station_list:\n",
      "        station_data = {\n",
      "            'station_id': station_info['id'],\n",
      "            'station_name': station_info['stationName'],\n",
      "            'latitude': station_info['latitude'],\n",
      "            'longitude': station_info['longitude'],\n",
      "            'weekday': weekday_index < 5,\n",
      "            'hour': the_hour,\n",
      "            'minute': the_minute,\n",
      "            'total_docks': station_info['totalDocks'],\n",
      "            'available_docks': station_info['availableDocks'],\n",
      "            'available_bikes': station_info['availableBikes']\n",
      "        }\n",
      "        yield station_data, 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that I'm grouping by station_name as well as id... That's more for convenience than anything. I don't believe that will have changed in the time since I've started collecting data. I want it in the key just to make sure it all gets into my final results.\n",
      "\n",
      "### Step 2: Sum up the tallies for each unique key\n",
      "MRJob ensures that when it calls our reduce function that we get all of the tallies that were associated with a given key all at once."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def group_minutes_by_station(self, station_data, tallies):\n",
      "    tally_total = sum(tallies)\n",
      "    yield station_data, tally_total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When MRJob calls this, all I need to do is sum up all of the tallies for this key and then I know how many times this exact occurence of weekday, hour, minute, etc. happened."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 3: Reshaping the data\n",
      "Now I'll have a perfectly valid representation of how often I see these results but it's a really long linear list. Instead I really want it all organized into more of a ferquency distribution one might expect in a histogram.\n",
      "\n",
      "To do that, I need to move the total_docks, available_docks, and available_bikes out of the key and move it into value. This way in my next reduce step, I'll be able to take all of the different occurences of those values and add their tallies into the appropriate bucket.\n",
      "\n",
      "This is the mapping step in this transformation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def organize_into_buckets(self, station_data, tally_total):\n",
      "    metrics = {\n",
      "        'total_docks': station_data['total_docks'],\n",
      "        'available_docks': station_data['available_docks'],\n",
      "        'available_bikes': station_data['available_bikes'],\n",
      "        'occurences': tally_total\n",
      "    }\n",
      "    del station_data['total_docks']\n",
      "    del station_data['available_docks']\n",
      "    del station_data['available_bikes']\n",
      "    yield station_data, metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 4: Create the frequency distributions\n",
      "Now we'll take this new data and since it will be grouped by everything BUT the dock and bike availability, this function should be called once for each bike station, for each minute in each hour and once on weekdays and once again on weekends.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_frequency_distributions(self, station_data, all_station_metrics):\n",
      "    available_bikes_frequencies = [0]*100\n",
      "    total_docks_frequencies = [0]*100\n",
      "    available_docks_frequencies = [0]*100\n",
      "    for station_metric in all_station_metrics:\n",
      "        available_bikes_frequencies[station_metric['available_bikes']] += station_metric['occurences']\n",
      "        total_docks_frequencies[station_metric['total_docks']] += station_metric['occurences']\n",
      "        available_docks_frequencies[station_metric['available_docks']] += station_metric['occurences']\n",
      "    station_data['available_bikes'] = available_bikes_frequencies\n",
      "    station_data['available_docks'] = available_docks_frequencies\n",
      "    station_data['total_docks'] = total_docks_frequencies\n",
      "    yield None, station_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since this is my last step, note how I'm returning 'None' and also combining all of my data together into a single object. Because I set my output protocol to be a JSONValueProtocol MRJob will automatically serialize my Python dictionary into JSON and out that to my results files.\n",
      "\n",
      "Here's the whole thing at once:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mrjob.job import MRJob\n",
      "from mrjob import protocol\n",
      "from datetime import datetime\n",
      "\n",
      "class AggregateDivvyFrequencies(MRJob):\n",
      "    INPUT_PROTOCOL = protocol.JSONValueProtocol\n",
      "    OUTPUT_PROTOCOL = protocol.JSONValueProtocol\n",
      "\n",
      "    def steps(self):\n",
      "        return [self.mr(mapper=self.organize_station_data_and_tally,\n",
      "                        reducer=self.group_minutes_by_station),\n",
      "                self.mr(mapper=self.organize_into_buckets,\n",
      "                        reducer=self.create_frequency_distributions)]\n",
      "\n",
      "    def organize_station_data_and_tally(self, _, station_update):\n",
      "        execution_time_text = station_update['executionTime']\n",
      "        the_datetime = datetime.strptime(execution_time_text, '%Y-%m-%d %I:%M:%S %p')\n",
      "        the_hour = the_datetime.hour\n",
      "        the_minute = the_datetime.minute\n",
      "        weekday_index = the_datetime.weekday()\n",
      "        weekday_map = [\n",
      "            'Monday',\n",
      "            'Tuesday',\n",
      "            'Wednesday',\n",
      "            'Thursday',\n",
      "            'Friday',\n",
      "            'Saturday',\n",
      "            'Sunday'\n",
      "        ]\n",
      "        station_list = station_update['stationBeanList']\n",
      "        for station_info in station_list:\n",
      "            station_data = {\n",
      "                'station_id': station_info['id'],\n",
      "                'station_name': station_info['stationName'],\n",
      "                'latitude': station_info['latitude'],\n",
      "                'longitude': station_info['longitude'],\n",
      "                'weekday': weekday_index < 5,\n",
      "                'hour': the_hour,\n",
      "                'minute': the_minute,\n",
      "                'total_docks': station_info['totalDocks'],\n",
      "                'available_docks': station_info['availableDocks'],\n",
      "                'available_bikes': station_info['availableBikes']\n",
      "            }\n",
      "            yield station_data, 1\n",
      "\n",
      "    def group_minutes_by_station(self, station_data, tallies):\n",
      "        tally_total = sum(tallies)\n",
      "        yield station_data, tally_total\n",
      "\n",
      "    def organize_into_buckets(self, station_data, tally_total):\n",
      "        metrics = {\n",
      "            'total_docks': station_data['total_docks'],\n",
      "            'available_docks': station_data['available_docks'],\n",
      "            'available_bikes': station_data['available_bikes'],\n",
      "            'occurences': tally_total\n",
      "        }\n",
      "\n",
      "        del station_data['total_docks']\n",
      "        del station_data['available_docks']\n",
      "        del station_data['available_bikes']\n",
      "        \n",
      "        yield station_data, metrics\n",
      "\n",
      "    def create_frequency_distributions(self, station_data, all_station_metrics):\n",
      "        available_bikes_frequencies = [0]*100\n",
      "        total_docks_frequencies = [0]*100\n",
      "        available_docks_frequencies = [0]*100\n",
      "        for station_metric in all_station_metrics:\n",
      "            available_bikes_frequencies[station_metric['available_bikes']] += station_metric['occurences']\n",
      "            total_docks_frequencies[station_metric['total_docks']] += station_metric['occurences']\n",
      "            available_docks_frequencies[station_metric['available_docks']] += station_metric['occurences']\n",
      "        station_data['available_bikes'] = available_bikes_frequencies\n",
      "        station_data['available_docks'] = available_docks_frequencies\n",
      "        station_data['total_docks'] = total_docks_frequencies\n",
      "        yield None, station_data\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    AggregateDivvyFrequencies.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Debugging and running it\n",
      "\n",
      "As I said, I download some data from S3 to run through this and make sure I didn't make any bone headed mistakes. I can invoke a local run with this command in Terminal:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "python aggregate_frequencies.py ./test_data/day_201311* --output-dir ./results --no-output"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then when I'm happy I have it working, I can run the entire thing on EMR like so:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "python aggregate_frequencies.py s3://bozo-divvy/day-log/ --output-dir s3://bozo-divvy/day-log-frequency-results --no-output -r emr"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a great chance to point out an awesome feature of MRJob. See above how I specify the output directory as '--output-dir'? I love how I don't have to specify that I'm pulling data from S3 for the EMR version. MRJob just infers it from the path I provide to it. I also have a MRJob config file that I keep in one of MRJob's default paths so I don't have to pass it in as a parameter every time I run a job. More about that here: [http://pythonhosted.org/mrjob/configs.html](http://pythonhosted.org/mrjob/configs.html)\n",
      "\n",
      "Here's an example of what the results look like:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "{\"hour\": 0, \"available_bikes\": [0, 1, 7, 11, 8, 9, 10, 16, 5, 2, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"station_id\": 247, \"total_docks\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"longitude\": -87.580714999999998, \"available_docks\": [0, 0, 1, 0, 0, 7, 2, 5, 16, 10, 9, 8, 11, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"latitude\": 41.795211999999999, \"station_name\": \"Shore Drive & 55th St\", \"minute\": 0, \"weekday\": true}\t\n",
      "{\"hour\": 0, \"available_bikes\": [0, 0, 3, 2, 6, 8, 3, 4, 0, 3, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"station_id\": 247, \"total_docks\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"longitude\": -87.580714999999998, \"available_docks\": [0, 0, 1, 1, 0, 1, 3, 0, 4, 3, 8, 6, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"latitude\": 41.795211999999999, \"station_name\": \"Shore Drive & 55th St\", \"minute\": 10, \"weekday\": false}\t\n",
      "{\"hour\": 0, \"available_bikes\": [0, 0, 4, 1, 5, 5, 3, 5, 1, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"station_id\": 247, \"total_docks\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"longitude\": -87.580714999999998, \"available_docks\": [0, 0, 1, 0, 0, 1, 3, 1, 5, 3, 5, 5, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"latitude\": 41.795211999999999, \"station_name\": \"Shore Drive & 55th St\", \"minute\": 11, \"weekday\": false}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Modeling with our data\n",
      "\n",
      "Now to get back to our original question. I want to get to being able to point at a given station on the weekday or weekend at a given moment in time and ask how likely I am to find a bike available.\n",
      "\n",
      "First I'll need to find the ID of the station that's closest to me."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "results_filepath = '/Users/justin/Documents/Code/divvy_probability/results.txt'\n",
      "my_longitude = -87.664362\n",
      "my_latitude = 42.007758\n",
      "closest_station = None\n",
      "closest_station_distance = float('inf')\n",
      "\n",
      "with open(results_filepath, 'r') as results_file:\n",
      "    for line in results_file:\n",
      "        station_data = json.loads(line)\n",
      "        station_latitude = station_data['latitude']\n",
      "        station_logitude = station_data['longitude']\n",
      "        distance = math.sqrt(math.pow(station_latitude-my_latitude,2)+math.pow(station_logitude-my_longitude,2))\n",
      "        if distance < closest_station_distance:\n",
      "            closest_station = {'id': station_data['station_id'], 'name': station_data['station_name']}\n",
      "            closest_station_distance = distance\n",
      "            \n",
      "print closest_station      \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'id': 294, 'name': u'Broadway & Berwyn Ave'}\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seems legit.\n",
      "\n",
      "Right now it's 12:58 PM on a weekend... Let's see how likely it is there are bikes available."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "matching_station = None\n",
      "current_frequencies = numpy.array([])\n",
      "current_probabilities = numpy.array([])\n",
      "with open(results_filepath, 'r') as results_file:\n",
      "    for line in results_file:\n",
      "        station_data = json.loads(line)\n",
      "        if station_data['station_id'] == closest_station['id'] and station_data['weekday']==False and station_data['hour'] == 13 and station_data['minute']==5:\n",
      "            matching_station = station_data\n",
      "            current_frequencies = numpy.array(station_data['available_bikes'])\n",
      "            break\n",
      "current_probabilities = current_frequencies / (1.0*sum(current_frequencies))\n",
      "\n",
      "print \"Probability of number of bikes available\"\n",
      "for i in range(0,16):\n",
      "    print str(i) + ': ' + str(100*current_probabilities[i]) + '%'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Probability of number of bikes available\n",
        "0: 0.0%\n",
        "1: 0.0%\n",
        "2: 0.0%\n",
        "3: 6.25%\n",
        "4: 18.75%\n",
        "5: 15.625%\n",
        "6: 21.875%\n",
        "7: 6.25%\n",
        "8: 15.625%\n",
        "9: 0.0%\n",
        "10: 3.125%\n",
        "11: 3.125%\n",
        "12: 3.125%\n",
        "13: 3.125%\n",
        "14: 3.125%\n",
        "15: 0.0%\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we assume the frequencies are directly interpretable as probabilities then there is practically a 100% chance there will be a bike available. There is also an 84.375% chance that there are between 3 to 8 bikes available.\n",
      "\n",
      "Looking at this, it says it's IMPOSSIBLE that there will be no bikes available. That seems optimistic to me. Let's see what the probability distribution looks like for there NOT being a bike available."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "\n",
      "samples = map(lambda x: math.floor(100*numpy.random.beta(1, 1+sum(current_frequencies))), numpy.arange(0,1000))\n",
      "histogram = np.histogram(samples, bins=range(0,100))\n",
      "likelihood_no_bikes = histogram[0]/(1.0*sum(histogram[0]))\n",
      "print likelihood_no_bikes\n",
      "import matplotlib.pyplot as plt\n",
      "plt.hist(samples, bins=range(0,100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.289  0.205  0.161  0.11   0.068  0.059  0.032  0.025  0.017  0.009\n",
        "  0.006  0.007  0.003  0.003  0.001  0.002  0.     0.001  0.002  0.     0.\n",
        "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
        "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
        "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
        "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
        "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
        "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
        "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
        "  0.   ]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "(array([289, 205, 161, 110,  68,  59,  32,  25,  17,   9,   6,   7,   3,\n",
        "         3,   1,   2,   0,   1,   2,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0,   0,   0,   0,   0,   0]),\n",
        " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
        "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
        "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
        "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
        "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
        "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
        " <a list of 99 Patch objects>)"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD9CAYAAABdoNd6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x111b63e90>"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this we can count up the frequencies of our hypotheses and conclude that a credible likelihood interval is a 0-7% chance that there will be no bikes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now what? Well the next step will be to enable people to explore this data. The current form of this data doesn't do much to help us with this task. Instead I'd like to make this something that my Fianc\u00e9e would be interested to see. If I can make a solid visualization out of this, I think that might do the trick. \n",
      "\n",
      "Sounds like a great topic for my next blog post. Thanks for reading!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}